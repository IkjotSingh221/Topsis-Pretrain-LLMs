{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "*Importing neccessary libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "hugging_face_api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from datasets import load_metric\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialising the LLMs\n",
    "- codellama/CodeLlama-7b-hf\n",
    "- mistralai/Mistral-7B-v0.1\n",
    "- h2oai/h2o-danube-1.8b-chat\n",
    "- HuggingFaceH4/zephyr-7b-beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm1 = HuggingFaceHub(huggingfacehub_api_token=hugging_face_api_token,\n",
    "                     repo_id = \"codellama/CodeLlama-7b-hf\",\n",
    "                     model_kwargs = {\"temperature\" : 0.7,\"max_length\": 150}\n",
    ")\n",
    "llm2 = HuggingFaceHub(huggingfacehub_api_token=hugging_face_api_token,\n",
    "                     repo_id = \"mistralai/Mistral-7B-v0.1\",\n",
    "                     model_kwargs = {\"temperature\" : 0.7,\"max_length\": 150}\n",
    ")\n",
    "llm3 = HuggingFaceHub(huggingfacehub_api_token=hugging_face_api_token,\n",
    "                     repo_id = \"h2oai/h2o-danube-1.8b-chat\",\n",
    "                     model_kwargs = {\"temperature\" : 0.7,\"max_length\": 150}\n",
    ")\n",
    "llm4 = HuggingFaceHub(huggingfacehub_api_token=hugging_face_api_token,\n",
    "                     repo_id = \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "                     model_kwargs = {\"temperature\" : 0.7,\"max_length\": 150}\n",
    ")\n",
    "\n",
    "llm=[llm1,llm2,llm3,llm4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Dataset\n",
    "*Creating a sample dataset to evaluate the LLMs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = [    \n",
    "    {\"prompt\": \"Discuss the importance of early childhood education.\", \"expected_output\": \"Early childhood education lays the foundation for lifelong learning and development.\"},\n",
    "    {\"prompt\": \"Explain the concept of differentiation in education.\", \"expected_output\": \"Differentiation in education involves adapting instruction to meet the diverse needs of students.\"},\n",
    "    {\"prompt\": \"Discuss the principles of democracy and their significance in modern politics.\", \"expected_output\": \"Democracy is a system of government based on the principles of popular sovereignty, political equality, and majority rule, with protections for minority rights.\"},\n",
    "    {\"prompt\": \"Describe the rules and objective of soccer.\", \"expected_output\": \"Soccer is a team sport played between two teams of eleven players each, with the objective of scoring goals by kicking the ball into the opposing team's goal.\"},\n",
    "    {\"prompt\": \"Discuss the health benefits of regular physical activity and sports participation.\", \"expected_output\": \"Regular physical activity and sports participation promote cardiovascular health, strength, endurance, and overall well-being.\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Scores\n",
    "*Calculating and creating a dataframe of scores*\n",
    "*Evaluation parameters:*\n",
    "- Bert Score (precision, recall, f1 score)\n",
    "- Bleu Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ikjot singh\\AppData\\Local\\Temp\\ipykernel_14156\\2541563621.py:2: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  bertscore_metric = load_metric('bertscore')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "binary_path: c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "CUDA SETUP: Loading binary c:\\Python311\\Lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=[\"Prompt\",\"Model\", \"Precision\", \"Recall\", \"F1_Score\", \"Bleu_Score\"])\n",
    "bertscore_metric = load_metric('bertscore')\n",
    "j=0\n",
    "i=0\n",
    "for e in evaluation_dataset:\n",
    "    for l,model in enumerate(llm,start=1):  \n",
    "        prompt = e[\"prompt\"]        \n",
    "        expected_output = e[\"expected_output\"]\n",
    "        generated_text = model(prompt)\n",
    "        bleu_score = sentence_bleu([expected_output.split()], generated_text.split(), smoothing_function=SmoothingFunction().method1)\n",
    "        bert_scores = bertscore_metric.compute(predictions=[generated_text], references=[expected_output], lang=\"en\")\n",
    "        precision = bert_scores['precision'][0]\n",
    "        recall = bert_scores['recall'][0]\n",
    "        f1 = bert_scores['f1'][0]\n",
    "        df.loc[j] = [\"Prompt {}\".format(i+1),\"LLM {}\".format(l), precision, recall, f1, bleu_score]\n",
    "        j+=1\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prompt</th>\n",
       "      <th>Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Bleu_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Prompt 1</td>\n",
       "      <td>LLM 1</td>\n",
       "      <td>0.819590</td>\n",
       "      <td>0.871123</td>\n",
       "      <td>0.844571</td>\n",
       "      <td>0.005771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prompt 1</td>\n",
       "      <td>LLM 2</td>\n",
       "      <td>0.818480</td>\n",
       "      <td>0.879037</td>\n",
       "      <td>0.847679</td>\n",
       "      <td>0.005587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Prompt 1</td>\n",
       "      <td>LLM 3</td>\n",
       "      <td>0.842658</td>\n",
       "      <td>0.919741</td>\n",
       "      <td>0.879514</td>\n",
       "      <td>0.011076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Prompt 1</td>\n",
       "      <td>LLM 4</td>\n",
       "      <td>0.848720</td>\n",
       "      <td>0.930715</td>\n",
       "      <td>0.887828</td>\n",
       "      <td>0.022944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Prompt 2</td>\n",
       "      <td>LLM 1</td>\n",
       "      <td>0.819763</td>\n",
       "      <td>0.868085</td>\n",
       "      <td>0.843232</td>\n",
       "      <td>0.003568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Prompt 2</td>\n",
       "      <td>LLM 2</td>\n",
       "      <td>0.847417</td>\n",
       "      <td>0.925050</td>\n",
       "      <td>0.884534</td>\n",
       "      <td>0.006000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Prompt 2</td>\n",
       "      <td>LLM 3</td>\n",
       "      <td>0.842026</td>\n",
       "      <td>0.936841</td>\n",
       "      <td>0.886906</td>\n",
       "      <td>0.017695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Prompt 2</td>\n",
       "      <td>LLM 4</td>\n",
       "      <td>0.859235</td>\n",
       "      <td>0.922353</td>\n",
       "      <td>0.889676</td>\n",
       "      <td>0.009524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Prompt 3</td>\n",
       "      <td>LLM 1</td>\n",
       "      <td>0.801087</td>\n",
       "      <td>0.830884</td>\n",
       "      <td>0.815713</td>\n",
       "      <td>0.004625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Prompt 3</td>\n",
       "      <td>LLM 2</td>\n",
       "      <td>0.861482</td>\n",
       "      <td>0.905730</td>\n",
       "      <td>0.883052</td>\n",
       "      <td>0.017144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Prompt 3</td>\n",
       "      <td>LLM 3</td>\n",
       "      <td>0.861571</td>\n",
       "      <td>0.910248</td>\n",
       "      <td>0.885241</td>\n",
       "      <td>0.062412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Prompt 3</td>\n",
       "      <td>LLM 4</td>\n",
       "      <td>0.841237</td>\n",
       "      <td>0.860267</td>\n",
       "      <td>0.850645</td>\n",
       "      <td>0.005308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Prompt 4</td>\n",
       "      <td>LLM 1</td>\n",
       "      <td>0.803796</td>\n",
       "      <td>0.818664</td>\n",
       "      <td>0.811161</td>\n",
       "      <td>0.004927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Prompt 4</td>\n",
       "      <td>LLM 2</td>\n",
       "      <td>0.879523</td>\n",
       "      <td>0.926622</td>\n",
       "      <td>0.902458</td>\n",
       "      <td>0.106854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Prompt 4</td>\n",
       "      <td>LLM 3</td>\n",
       "      <td>0.839356</td>\n",
       "      <td>0.901314</td>\n",
       "      <td>0.869233</td>\n",
       "      <td>0.008846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Prompt 4</td>\n",
       "      <td>LLM 4</td>\n",
       "      <td>0.865548</td>\n",
       "      <td>0.951331</td>\n",
       "      <td>0.906415</td>\n",
       "      <td>0.147593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Prompt 5</td>\n",
       "      <td>LLM 1</td>\n",
       "      <td>0.819876</td>\n",
       "      <td>0.872802</td>\n",
       "      <td>0.845512</td>\n",
       "      <td>0.002453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Prompt 5</td>\n",
       "      <td>LLM 2</td>\n",
       "      <td>0.851075</td>\n",
       "      <td>0.890580</td>\n",
       "      <td>0.870379</td>\n",
       "      <td>0.005488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Prompt 5</td>\n",
       "      <td>LLM 3</td>\n",
       "      <td>0.846601</td>\n",
       "      <td>0.931021</td>\n",
       "      <td>0.886807</td>\n",
       "      <td>0.059889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Prompt 5</td>\n",
       "      <td>LLM 4</td>\n",
       "      <td>0.852721</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.049284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prompt  Model  Precision    Recall  F1_Score  Bleu_Score\n",
       "0   Prompt 1  LLM 1   0.819590  0.871123  0.844571    0.005771\n",
       "1   Prompt 1  LLM 2   0.818480  0.879037  0.847679    0.005587\n",
       "2   Prompt 1  LLM 3   0.842658  0.919741  0.879514    0.011076\n",
       "3   Prompt 1  LLM 4   0.848720  0.930715  0.887828    0.022944\n",
       "4   Prompt 2  LLM 1   0.819763  0.868085  0.843232    0.003568\n",
       "5   Prompt 2  LLM 2   0.847417  0.925050  0.884534    0.006000\n",
       "6   Prompt 2  LLM 3   0.842026  0.936841  0.886906    0.017695\n",
       "7   Prompt 2  LLM 4   0.859235  0.922353  0.889676    0.009524\n",
       "8   Prompt 3  LLM 1   0.801087  0.830884  0.815713    0.004625\n",
       "9   Prompt 3  LLM 2   0.861482  0.905730  0.883052    0.017144\n",
       "10  Prompt 3  LLM 3   0.861571  0.910248  0.885241    0.062412\n",
       "11  Prompt 3  LLM 4   0.841237  0.860267  0.850645    0.005308\n",
       "12  Prompt 4  LLM 1   0.803796  0.818664  0.811161    0.004927\n",
       "13  Prompt 4  LLM 2   0.879523  0.926622  0.902458    0.106854\n",
       "14  Prompt 4  LLM 3   0.839356  0.901314  0.869233    0.008846\n",
       "15  Prompt 4  LLM 4   0.865548  0.951331  0.906415    0.147593\n",
       "16  Prompt 5  LLM 1   0.819876  0.872802  0.845512    0.002453\n",
       "17  Prompt 5  LLM 2   0.851075  0.890580  0.870379    0.005488\n",
       "18  Prompt 5  LLM 3   0.846601  0.931021  0.886807    0.059889\n",
       "19  Prompt 5  LLM 4   0.852721  0.883681  0.867925    0.049284"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topsis \n",
    "*Weighted normalisation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def wt_norm(df,weights):\n",
    "  \n",
    "  df = df.div(df.apply(lambda x: x*2).apply(sum).apply(lambda x: x*0.5))\n",
    "  df = df.mul(weights)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Topsis Function Definition*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def topsis(input,weights, impacts):\n",
    "    df=input.drop(['Prompt','Model'],axis=1)\n",
    "    df = wt_norm(df,weights)\n",
    "    b=[]\n",
    "    w=[]\n",
    "    for i in range(len(impacts)):\n",
    "        if impacts[i]=='+':\n",
    "            b.append(max(df.iloc[:,i]))\n",
    "            w.append(min(df.iloc[:,i]))\n",
    "        else:\n",
    "            b.append(min(df.iloc[:,i]))\n",
    "            w.append(max(df.iloc[:,i]))\n",
    "\n",
    "    # Calculating performance\n",
    "    s_best=[]\n",
    "    s_worst=[]\n",
    "    for index, row in df.iterrows():\n",
    "        s_best.append((sum((row - b) ** 2) ** 0.5) * 0.5)\n",
    "        s_worst.append((sum((row - w) ** 2) ** 0.5) * 0.5)\n",
    "    s_total = [i + j for i, j in zip(s_worst, s_best)]\n",
    "    performance = [i / j if j != 0 else 0 for i, j in zip(s_worst, s_total)]\n",
    "    df['Topsis Score'] = performance\n",
    "    \n",
    "    # Ranking\n",
    "    sorted_array = df.loc[:,'Topsis Score'].argsort()\n",
    "    ranks = np.empty_like(sorted_array)\n",
    "    ranks[sorted_array] = np.arange(len(sorted_array))\n",
    "    n=len(sorted_array)\n",
    "    ranks = [n-i for i in ranks]\n",
    "    df.loc[:,'Rank'] = ranks\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Inputs for Topsis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [df[0:4 ], df[4:8], df[8:12], df[12:16], df[16:20]]\n",
    "weights = [1,1,1,1]\n",
    "impacts = [\"+\", \"+\", \"+\", \"+\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output\n",
    "*Ranking the models  based on their performance score calculated using Topsis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt  1 : Discuss the importance of early childhood education.\n",
      "   Precision    Recall  F1_Score  Bleu_Score  Topsis Score  Rank\n",
      "0   0.246164  0.241937  0.244125    0.127179      0.010628     3\n",
      "1   0.245831  0.244135  0.245023    0.123120      0.006161     4\n",
      "2   0.253092  0.255440  0.254225    0.244090      0.318706     2\n",
      "3   0.254913  0.258488  0.256628    0.505612      1.000000     1\n",
      "\n",
      "\n",
      "Prompt  2 : Explain the concept of differentiation in education.\n",
      "   Precision    Recall  F1_Score  Bleu_Score  Topsis Score  Rank\n",
      "4   0.243366  0.237680  0.240625    0.096982      0.000000     4\n",
      "5   0.251576  0.253277  0.252410    0.163093      0.179213     3\n",
      "6   0.249975  0.256505  0.253087    0.481029      0.986742     1\n",
      "7   0.255084  0.252538  0.253878    0.258896      0.424021     2\n",
      "\n",
      "\n",
      "Prompt  3 : Discuss the principles of democracy and their significance in modern politics.\n",
      "    Precision    Recall  F1_Score  Bleu_Score  Topsis Score  Rank\n",
      "8    0.238038  0.236913  0.237495    0.051687      0.000000     4\n",
      "9    0.255984  0.258254  0.257101    0.191570      0.221559     2\n",
      "10   0.256010  0.259542  0.257738    0.697424      1.000000     1\n",
      "11   0.249968  0.245291  0.247666    0.059319      0.029412     3\n",
      "\n",
      "\n",
      "Prompt  4 : Describe the rules and objective of soccer.\n",
      "    Precision    Recall  F1_Score  Bleu_Score  Topsis Score  Rank\n",
      "12   0.237232  0.227537  0.232473    0.018369      0.000000     4\n",
      "13   0.259582  0.257543  0.258638    0.398382      0.715691     2\n",
      "14   0.247728  0.250509  0.249116    0.032980      0.060928     3\n",
      "15   0.255458  0.264411  0.259772    0.550269      0.992338     1\n",
      "\n",
      "\n",
      "Prompt  5 : Discuss the health benefits of regular physical activity and sports participation.\n",
      "    Precision    Recall  F1_Score  Bleu_Score  Topsis Score  Rank\n",
      "16   0.243267  0.243930  0.243620    0.020946      0.000000     4\n",
      "17   0.252524  0.248898  0.250785    0.046859      0.058485     3\n",
      "18   0.251197  0.260201  0.255518    0.511372      0.996315     1\n",
      "19   0.253012  0.246970  0.250078    0.420823      0.813573     2\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(\"Prompt \",i+1,\":\",evaluation_dataset[i]['prompt'])\n",
    "    res = topsis(inputs[i], weights, impacts)\n",
    "    \n",
    "    print(res)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Domain | Best Model | Model Name |\n",
    "|-----------------|-----------------|-----------------|\n",
    "| Education    | M4,M3   |  h2oai/h2o-danube-1.8b-chat , HuggingFaceH4/zephyr-7b-beta    |\n",
    "| Sports    | M4,M3    |  h2oai/h2o-danube-1.8b-chat , HuggingFaceH4/zephyr-7b-beta   |\n",
    "| Politics    | M3    |  h2oai/h2o-danube-1.8b-chat   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
